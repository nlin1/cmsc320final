{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "import praw\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, string\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment will analyze the frequency of words that appear in the most positively and negatively scored comments from Reddit. The two nltk package downloads are required to tag individual words to their part of speech. This allows us to filter the words we do not believe are relevant to a post's popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word maps will contain every instance of a valid word within the comment texts. The pos_map is a list of parts of speech that will be included in the filtering for our word table.\n",
    "\n",
    "List with examples\n",
    "<ul>\n",
    "  <li>JJ adjective 'big'\n",
    "</li>\n",
    "  <li>JJR adjective, comparative 'bigger'\n",
    "</li>\n",
    "  <li>JJS adjective, superlative 'biggest'\n",
    "</li>\n",
    "  <li>NN noun, singular 'desk'\n",
    "</li>\n",
    "  <li>NNS noun plural 'desks'\n",
    "</li>\n",
    "  <li>NNP proper noun, singular 'Harrison'\n",
    "</li>\n",
    "   <li>NNPS proper noun, plural 'Americans'\n",
    "</li>\n",
    "  <li>RB adverb very, silently,\n",
    "</li>\n",
    "  <li>RBR adverb, comparative better\n",
    "</li>\n",
    "   <li>RBS adverb, superlative best\n",
    "</li>\n",
    "  <li>RP particle give up\n",
    "</li>\n",
    "  <li>UH interjection errrrrrrrm\n",
    "</li>\n",
    "   <li>VB verb, base form take\n",
    "</li>\n",
    "  <li>VBD verb, past tense took\n",
    "</li>\n",
    "  <li>VBG verb, gerund/present participle taking\n",
    "</li>\n",
    "   <li>VBN verb, past participle taken\n",
    "</li>\n",
    "  <li>VBP verb, sing. present, non-3d take\n",
    "</li>\n",
    "  <li>VBZ verb, 3rd person sing. present takes\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "The regular expression will filter out all nonalphanumeric characters. The natural language toolkit's tokenizer splits words into tokens based on their criteria for parts of speech. This separates words that have nonalphanumeric characters between them like a hyphen. This is not favorable, since it also allows duplicate words with differing capitalizations within the word. The regular expression will ensure that all words are split on space, and each word does not contain nonsense characters. The filter for these parts of speech is used to more accurately select real english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_map = []\n",
    "neg_word_map = []\n",
    "pos_map = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"RB\", \"RBR\", \"RBS\", \"RP\",\"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "pattern = re.compile('[\\W_]+', re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code iterates through the positive comment table and copies instances of valid words to the pos_word_map as a tuple of it and the corresponding comment score. This will allow us to measure the relative score of a word in relation to the score of the comments it appears in. Every single word within the text will be assigned the point value of the post it came from, and words that appear multiple times within the text will be weighted with multiple of these assignments. This will increase the positive or negative predictor score of words based on their frequency in high or low scoring points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in posjson.iterrows():\n",
    "    text = row.text\n",
    "    new_text = text.split()\n",
    "    results = []\n",
    "    score = row.score\n",
    "    for word in new_text:\n",
    "        temp = pattern.sub('', word).lower()\n",
    "        if temp != '':\n",
    "            results.append((temp,score))\n",
    "    pos_word_map.extend(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code sections are intermediary steps in creating a cleaned up DataFrame which we can work with. The DataFrames are manipulated in order to translate the information from a word, score tuple Series into a DataFrame. Two DataFrames are then needed in order to cleanly sum up the frequency of each word within the entire corpus of reddit comments for the json records as well as the associated score for each word. The following show the process as these mappings are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame for the word,score tuples\n",
    "pos_tokens_df = pd.DataFrame(pos_word_map, columns = ['word', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame with the frequency of each word in the total pos_word_map\n",
    "pos_count_df = pd.DataFrame(pos_tokens_df.word.value_counts())\n",
    "pos_count_df.reset_index(level=0, inplace=True)\n",
    "pos_count_df.rename(columns={\"index\":\"word\", \"word\":\"frequency\"}, inplace=True)\n",
    "pos_count_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame for the positively scored words where the raw score is the sum of all of the instances of a word's \n",
    "#post score\n",
    "pos_score_df = pd.DataFrame(pos_word_map)\n",
    "pos_score_df = pd.DataFrame(pos_score_df)\n",
    "pos_score_df.reset_index(level=0, inplace=True)\n",
    "pos_score_df = pd.DataFrame(pos_score_df.groupby(0)[1].sum())\n",
    "pos_score_df.reset_index(level=0, inplace=True)\n",
    "pos_score_df.rename(columns={0:\"word\",1:\"raw_score\"},inplace=True)\n",
    "pos_score_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the data contains a significant number of words that do not have any meaning in either English or in the context of comment score preditions. These words will be filtered out in a couple of steps. The frequency of each word was initially calculated in order to assist with removing words that appear to have no significance or words that may bias the data too much due to outlier posts that contain infrequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame with the average score of each word as the raw_score / word frequency\n",
    "pos_df = pos_count_df.set_index('word').join(pos_score_df.set_index('word'), rsuffix='_r')\n",
    "pos_df.reset_index(level=0, inplace=True)\n",
    "pos_df.rename(columns={\"score_r\":\"score\"}, inplace=True)\n",
    "pos_df['average_score'] = pos_df.raw_score / pos_df.frequency\n",
    "pos_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is in this step that the natural language toolkit is used in order to tag each word parsed in the previous segments with the part of speech the word belongs to. This allows us to choose only words from the list of parts of speech as valid entries in the DataFrame. Filtering out words that might either not be English or removing statistical outliers from appearing only a handful of times will make the data more accurate in predicting positively and negatively biased words. The frequency filter was set to 50 after determining a safeguard buffer for words that did not appear to make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame with the part of speech for each word added\n",
    "pos_df = pos_df[pos_df.frequency > 50]\n",
    "pos_tags = nltk.pos_tag(pos_df.word)\n",
    "pos_tags = pd.DataFrame(pos_tags)\n",
    "pos_df['part_of_speech'] = pos_tags[1]\n",
    "pos_df = pos_df[pos_df.part_of_speech.isin(pos_map)]\n",
    "pos_df = pd.DataFrame(pos_df)\n",
    "pos_df.reset_index(level=0,drop=True,inplace=True)\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to clean up the dataframes that are no longer in use, because the overhead for maintaining the dataframes and arrays in memory are too high. This may cause the kernal to terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup for memory\n",
    "del pos_tokens_df, pos_score_df, pos_word_map, pos_count_df, pos_tags\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code will replicate the upper section with regard to the most negatively scored comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in negjson.iterrows():\n",
    "    text = row.text\n",
    "    new_text = text.split()\n",
    "    results = []\n",
    "    score = row.score\n",
    "    for word in new_text:\n",
    "        temp = pattern.sub('', word).lower()\n",
    "        if temp != '':\n",
    "            results.append((temp,score))\n",
    "    neg_word_map.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame for the word,score tuples\n",
    "neg_tokens_df = pd.DataFrame(neg_word_map, columns = ['word', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame with the frequency of each word in the total pos_word_map\n",
    "neg_count_df = pd.DataFrame(neg_tokens_df.word.value_counts())\n",
    "neg_count_df.reset_index(level=0, inplace=True)\n",
    "neg_count_df.rename(columns={\"index\":\"word\", \"word\":\"frequency\"}, inplace=True)\n",
    "neg_count_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame for the negatively scored words where the raw score is the sum of all of the instances of a word's \n",
    "#post score\n",
    "neg_score_df = pd.DataFrame(neg_word_map)\n",
    "neg_score_df = pd.DataFrame(neg_score_df)\n",
    "neg_score_df.reset_index(level=0, inplace=True)\n",
    "neg_score_df = pd.DataFrame(neg_score_df.groupby(0)[1].sum())\n",
    "neg_score_df.reset_index(level=0, inplace=True)\n",
    "neg_score_df.rename(columns={0:\"word\",1:\"raw_score\"},inplace=True)\n",
    "neg_score_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame with the average score of each word as the raw_score / word frequency\n",
    "neg_df = neg_count_df.set_index('word').join(neg_score_df.set_index('word'), rsuffix='_r')\n",
    "neg_df.reset_index(level=0, inplace=True)\n",
    "neg_df.rename(columns={\"score_r\":\"score\"}, inplace=True)\n",
    "neg_df['average_score'] = neg_df.raw_score / neg_df.frequency\n",
    "neg_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a DataFrame with the part of speech for each word added\n",
    "neg_df = neg_df[neg_df.frequency > 50]\n",
    "neg_tags = nltk.pos_tag(neg_df.word)\n",
    "neg_tags = pd.DataFrame(neg_tags)\n",
    "neg_df['part_of_speech'] = neg_tags[1]\n",
    "neg_df = neg_df[neg_df.part_of_speech.isin(pos_map)]\n",
    "neg_df = pd.DataFrame(neg_df)\n",
    "neg_df.reset_index(level=0,drop=True,inplace=True)\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup for memory\n",
    "del neg_tokens_df, neg_score_df, neg_word_map, neg_count_df, neg_tags\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been organized for preprocessing, we take the sklearn modules to normalize and standardize the data on each of the numeric columns for both the pos_df and neg_df DataFrames. The most important columns to look at are the normalized and standardized average scores, since this represents the relationship between frequency of the word and the word's raw score. The normalization and standardization allow us to make statisical observations about certain words relative to other words in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_pos = pos_df.select_dtypes(include=[np.number])\n",
    "df_num_neg = neg_df.select_dtypes(include=[np.number])\n",
    "df_num_neg = df_num_neg.abs()\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "pos_df = pos_df.join(pd.DataFrame(min_max_scaler.fit_transform(df_num_pos), columns=df_num_pos.columns, \n",
    "                                  index=df_num_pos.index), rsuffix='_normalized')\n",
    "neg_df = neg_df.join(pd.DataFrame(min_max_scaler.fit_transform(df_num_neg), columns=df_num_neg.columns, \n",
    "                                  index=df_num_neg.index), rsuffix='_normalized')\n",
    "pos_df = pos_df.join(pd.DataFrame(standard_scaler.fit_transform(df_num_pos), columns=df_num_pos.columns, \n",
    "                                  index=df_num_pos.index), rsuffix='_standardized')\n",
    "neg_df = neg_df.join(pd.DataFrame(standard_scaler.fit_transform(df_num_neg), columns=df_num_neg.columns, \n",
    "                                  index=df_num_neg.index), rsuffix='_standardized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the normalized average score for both the words in the top 180,000 positive comments in green and the words in the top 180,000 negative comments in orange. The negatively scored words were normalized with their absolute value to present data that can be directly comparable to the positive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pos_df.average_score_normalized, shade=True, color='Green')\n",
    "sns.kdeplot(neg_df.average_score_normalized, shade=True, color='Orange')\n",
    "print(\"Positive Comments (Mean Normalized) : \" + str(pos_df.average_score_normalized.mean()))\n",
    "print(\"Positive Comments (Std Normalized)  : \" + str(pos_df.average_score_normalized.std()))\n",
    "print(\"Negative Comments (Mean Normalized) : \" + str(neg_df.average_score_normalized.mean()))\n",
    "print(\"Negative Comments (Std Normalized)  : \" + str(neg_df.average_score_normalized.std()))\n",
    "print(\"Difference (Mean Normalized)        : \" + str(pos_df.average_score_normalized.mean() - neg_df.average_score_normalized.mean()))\n",
    "print(\"Difference (Std Normalized)         : \" + str(pos_df.average_score_normalized.std() - neg_df.average_score_normalized.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second graph plots the standardized average scores for the positive and negative words. With the data standardized, we can now compare the words across positive and negative DataFrames to determine which words are the most positive, neutral, or negative by frequency and comment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pos_df.average_score_standardized, shade=True, color='Green')\n",
    "sns.kdeplot(neg_df.average_score_standardized, shade=True, color='Orange')\n",
    "print(\"Positive Comments (Mean Standardized) : \" + str(pos_df.average_score_standardized.mean()))\n",
    "print(\"Positive Comments (Std Standardized)  : \" + str(pos_df.average_score_standardized.std()))\n",
    "print(\"Negative Comments (Mean Standardized) : \" + str(neg_df.average_score_standardized.mean()))\n",
    "print(\"Negative Comments (Std Standardized)  : \" + str(neg_df.average_score_standardized.std()))\n",
    "print(\"Difference (Mean Standardized)        : \" + str(pos_df.average_score_standardized.mean() - neg_df.average_score_standardized.mean()))\n",
    "print(\"Difference (Std Standardized)         : \" + str(pos_df.average_score_standardized.std() - neg_df.average_score_standardized.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart shows the top 10 words by average standardized score for positive comments. The distribution skews long tails where the score is many standard deviations above the mean. This can be attributed to the fact that these data sets take a look at the top/lowest 180,000 comments by score out of a dataset that contains 380 million comments. By eliminating the majority of neutrally voted comments by absolute value, some words will be heavily biased due to their low frequency count in relation to their appearance in some highly scored points, whether they be positive or negative. This means that given analysis of enough data points it is highly likely for those top 9 words to fall in the rankings, since the top 9 also comprise entirely out of words that rank below 3700 on their frequency within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.sort_values(by=\"average_score_standardized\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_df.sort_values(by=\"average_score_standardized\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words from the positive and negative lists are combined to get the standardized score accounting for the difference between the positive commends and negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = pd.merge(left=pos_df, right=neg_df, left_on='word',right_on='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pos_df.loc[np.logical_not(pos_df.word.isin(neg_df.word))]\n",
    "right = neg_df.loc[np.logical_not(neg_df.word.isin(pos_df.word))]\n",
    "right[right.select_dtypes(include=[np.number]).columns] *= -1\n",
    "diff_join = pd.DataFrame(inner_join.word)\n",
    "diff_join['diff_standardized_score'] = inner_join.average_score_standardized_x - inner_join.average_score_standardized_y\n",
    "left.rename(columns={\"average_score_standardized\":\"diff_standardized_score\"}, inplace=True)\n",
    "right.rename(columns={\"average_score_standardized\":\"diff_standardized_score\"}, inplace=True)\n",
    "diff_df = pd.concat([diff_join, left[[\"word\",\"diff_standardized_score\"]], right[[\"word\",\"diff_standardized_score\"]]])\n",
    "diff_df.sort_values(by=\"diff_standardized_score\", ascending=False,inplace=True)\n",
    "diff_df.reset_index(level=0, drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have combined both standardized lists and accounted for the negative words as negative values, the distribution looks similar to the one above. The difference in negative values caused the graph to become mirrored on approxmiately x = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(diff_df.diff_standardized_score, shade=True, color='crimson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of the top 15 words that predict a positive scoring comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of the top 15 words that predict a negative scoring comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of the 10 words on both sides of standardized 0, which should indicate words that are neutral in predicting the positive or negative score of a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_df.loc[3559:3579]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
